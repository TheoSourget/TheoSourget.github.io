[{"categories":null,"contents":"During my projects, I like to build concrete applications from the research I conduct. I use them to demonstrate the capacity of a trained model or a pipeline I developed but also use the code in a more general purpose. To build these apps I use a combination of three tools: Streamlit, FastAPI, and Docker. In this post, I first show two examples of applications I\u0026rsquo;ve created, and then I present the tools with some simple examples of code.\nExample of applications UI for segmentation and generative models For this first example, I built an application to simplify the use of models trained to perform the segmentation of teeth in dental panoramics and to apply the first version of our algorithm to generate new panoramics, see more about this in our paper. I used this application during meetings with my supervisor as I believed it was better and easier than running classical Python scripts during demos.\nThe first component of the website lets the user upload a dental panoramic and decide the needed confidence for a pixel to be considered as a tooth. Then the model performs the segmentation when the user clicks on the “Launch segmentation” button.\nIn this project, we had access to a very limited amount of data. To mitigate this issue, we developed an algorithm to generate new panoramics based on the existing ones. The idea was to extract the teeth from the panoramics we had and add them to an empty one generated with an inpainting model. I developed an interface to let the user specify the teeth needed in the image as well as the age of the patient. As you can see in the image, it was the first version of the algorithm and the generated panoramics had some issues.\nAnnotation tool The other example is an annotation tool for scientific papers. I already presented this tool in this previous blog post about my paper [Citation needed] Data usage and citation practices in medical imaging conferences​. I built this app as other annotation tools didn\u0026rsquo;t meet my needs for annotating PDFs. This tool allows multiple annotators for the same files and keeps the PDF format. Moreover, we could also add the PDFs into different subgroups to ease the division of annotations among the annotators.\nPresentation of the tools I hope these two examples give you an idea of the wide range of applications you can create with these tools. I will now present them in a bit more detail.\nStreamlit Streamlit is an open-source Python library to build web applications. This library gives the possibility to data scientists to easily create applications from their data​​ using Python code with access to usual libraries such as pandas, numpy and matplotlib.\nLike any Python library, it can be installed easily with pip using the following commands (also creating a specific environment):\n#Create the environment and install streamlit conda create –n streamlit_demo python​ conda activate streamlit_demo pip install streamlit #Test the installation, the page should be accessible at http://localhost:8501/ streamlit hello Streamlit has multiple advantages, the first one is the very simple file structure compared to other libraries for web development in Python. For example, here is the file structure for the annotation tool I presented before without an API needed (you can also access the code here):\n├── data │ ├── DatasetPresence │ │ ├── annotations │ │ │ └── Théo.csv │ │ ├── papers_info.csv │ │ └── pdfs │ │ └── file_0 │ └── projects_info.csv ├── Dockerfile ├── Home.py ├── pages │ ├── 1_Create_A_Project.py │ ├── 2_Label_A_Project.py │ └── 3_Extract_Labels.py ├── README.md └── requirements.txt The main files are in the pages folder, they contain the code of your interface. This code can use any Python packages you may use in your analysis such as pandas or numpy but also some \u0026ldquo;magic commands\u0026rdquo; of Streamlit to display data. Similar to Jupyter notebooks, you can simply put the name of your variable and Streamlit will automatically render it on your page. You can for example use it to display your dataframe like this:\nimport streamlit as st import pandas as pd #Change the name of the tab in your browser st.set_page_config( page_title(\u0026#34;See our data\u0026#34;) ) st.title(\u0026#34;See our data\u0026#34;) df_pop_per_capital = pd.read_csv(\u0026#34;./res/pop_per_capital.csv\u0026#34;) #Use magic command to display the dataframe df_pop_per_capital Of course some basic components such as button, checkbox or text input are already accessible with some functions provided by Streamlit. For example, we can add a button in our previous example to download the csv:\nimport streamlit as st import pandas as pd #Change the name of the tab in your browser st.set_page_config( page_title(\u0026#34;See our data\u0026#34;) ) st.title(\u0026#34;See our data\u0026#34;) df_pop_per_capital = pd.read_csv(\u0026#34;./res/pop_per_capital.csv\u0026#34;) #Button to download the dataframe st.download_button( label=\u0026#34;Download data as CSV\u0026#34;, data=df_pop_per_capital, filename=f\u0026#34;df_pop_per_capital.csv\u0026#34;, mime=\u0026#34;text/csv\u0026#34; ) Streamlit offers many more components and is updated very frequently to add new features. You can find more guides and information on the official Streamlit documentation but also many examples of Streamlit applications from the community here.\nFastAPI FastAPI is a Python library and as you may guess from its name it is used to build your own API​. As Streamlit offers the possibility to use Python code it may be tempting to put everything in the same script, including data processing. However, for a better code and maintenance it is recommended to separate the code for the user interface (with Streamlit) and code handling the data which could also run your machine learning model (with FastAPI).\nAs for Streamlit the installation is very simple using pip:\n#Install with pip install fastapi uvicorn #Launch the API (with a python script named main.py) uvicorn main:app --host 0.0.0.0 --port 8000 –reload In your Python script, you will create functions which will be accessible through URLs as for any other API you may use in your project. Of course your API will contain code specific for your application but here is an example of a function with the main elements needed by FastAPI.\nYou can then use your API with the link you defined, you can even test it in your browser. For example, the previous code will return the first JSON if your request is valid or the second one in case of a bad parameter: As for Streamlit, the idea of this blog post is to present quickly the tools, I encourage you to go check the FastAPI documentation to learn more about it and start building your own application.\nDocker Docker will finally ease the deployment and start of our application. Docker would need its entire blog post to be presented and many people have already written one, so I encourage you to go read one of them for a more detailed description. To summarize it, Docker will automate the deployment of your application launching both the Streamlit code and your API. For an example of a complete application with Docker, you can check one of my GitHub repository, it has the following file structure with the back folder containing the FastAPI script and the front folder containing the Streamlit application:\n. ├── back │ ├── Dockerfile │ ├── main.py │ ├── __pycache__ │ └── requirements.txt ├── docker-compose.yml ├── front │ ├── Dockerfile │ ├── Home.py │ ├── pages │ ├── requirements.txt │ └── res ├── README.md ├── resources │ ├── dataframe_result.png │ ├── input_options.png │ ├── keywords_research.png │ └── statistics_display.png └── utils └── get_concepts.py With Docker you\u0026rsquo;ll first need to define your Dockerfile for your Streamlit application and your API. Here is a first example for Streamlit:\nFROM python:3.10 COPY requirements.txt app/requirements.txt WORKDIR /app RUN pip install -r requirements.txt COPY . /app EXPOSE 8501 ENTRYPOINT [\u0026#34;streamlit\u0026#34;,\u0026#34;run\u0026#34;] CMD [\u0026#34;Home.py\u0026#34;] And a seconde one for FastAPI:\nFROM python:3.10 COPY requirements.txt app/requirements.txt WORKDIR /app RUN pip install -r requirements.txt COPY . /app EXPOSE 8000 CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34; , \u0026#34;--reload\u0026#34;] Then your we\u0026rsquo;ll use docker-compose to combine the two with the following docker-compose.yml file:\nversion: \u0026#34;3.8\u0026#34; services: frontend: build: front ports: - 8501:8501 depends_on: - backend volumes: - ./front:/app backend: build: back ports: - 8000:8000 volumes: - ./back:/app Finally, you can launch your application with the command:\ndocker compose up -d --build Note that using Docker will have an impact on the URL you have to call to access your API. For instance, with the previous example, as my API container is called backend I\u0026rsquo;ll use it in the URL to query my API instead of localhost like this:\nurl = f\u0026#34;http://backend:8000/get_citations/openalex/?doi={doi}\u0026#34; request = requests.get(url) Conclusive remarks The objective of this blog post is not to give a complete tutorial for each tool but more to give you a first idea of their usage and advantages. I hope it motivated you to learn more about Streamlit, FastAPI and Docker and build your own applications. I really believe that building such applications can improve the quality of a demonstration but also give a new use to your research code with minimal efforts.\n","date":"April 20, 2025","hero":"/posts/other/create-your-own-app/images/preview.png","permalink":"https://theosourget.github.io/posts/other/create-your-own-app/","summary":"During my projects, I like to build concrete applications from the research I conduct. I use them to demonstrate the capacity of a trained model or a pipeline I developed but also use the code in a more general purpose. To build these apps I use a combination of three tools: Streamlit, FastAPI, and Docker. In this post, I first show two examples of applications I\u0026rsquo;ve created, and then I present the tools with some simple examples of code.","tags":null,"title":"Create your own applications with Streamlit, FastAPI and Docker​"},{"categories":null,"contents":"Click on the image below to see my oral session at MIDL 2024:\nNowadays, the evaluation of models heavily relies on publicly available datasets used as benchmarking. While this could be a nice thing for a fair comparison of different models, we also question the effect of the diversity or more precisely a potential lack of diversity in research papers when selecting the datasets. A gap has been observed between the results showcased by AI models in research and their adoption in clinical workflow, we hypothesise that this gap could partly be a result of an overfitting of research on these datasets and we wanted to evaluate their usage to know if some are more popular than others. While this could seem like a straightforward task we’ll see that because of some elements it turned out to be not so simple.\nIn our paper [Citation needed] Data usage and citation practices in medical imaging conferences, we present a pipeline collecting the citations and mentions of datasets in research papers. We applied this pipeline to track 20 datasets in papers from MIDL and MICCAI between 2013 and 2023. Finally, we developed an annotation tool for PDF. Both the pipeline and the annotation tool are fully open-source to potentially extend our study to other fields of application.\nPipeline presentation Overview Let\u0026rsquo;s first present an overview of the pipeline before going into more detail about each part. The main idea of this pipeline is to have a tool that can be extended easily to other datasets or venues, we therefore rely on public APIs and open-access tools. After defining the venues and datasets to be analysed, we can divide this pipeline into 4 parts: 1) We obtain information on papers from the venues. 2) We collect and convert PDFs of the full texts into a format more easily parsable. 3) We detect the presence of the datasets mostly through regular expression matching with dataset names, aliases or URLs. 4) We verify the efficiency of our tool on a subset of papers thanks to our PDF annotation tools.\nOpenAlex I’ll now present the first source of data for our work OpenAlex. OpenAlex is an open citation index tool, it’s basically an API gathering information from multiple sources such as Crossref, the Microsoft Academic Graph or arXiv for example and it standardized the information from all these sources. We considered other alternatives for this part such as OpenCitation or SemanticScholar and we concluded that OpenAlex was the best solution as it provided more information or had a better coverage of papers. I have to mention that when searching for paper information the first tool that comes to mind is probably Google Scholar but no API is available to obtain data from it.\nHere is part of the data from OpenAlex:\nFrom the API, we were interested in the abstract of the paper, the references and when available the link to the full text PDF. As you can see, the list of references is already computed by OpenAlex and as each paper has a unique OpenAlex ID we just have to match the ID for the datasets to the one present for the paper. You can also see how the PDF can be accessed from OpenAlex, it is however not always possible.\nCustom scraping tool To deal with this issue we added the possibility of using an additional scraping tool to gather PDFs, in our case it was especially useful for the papers from MIDL that were poorly present in the API and for which the PMLR website allows a fairly easy scraping. To be more concrete we defined a scraping tool with scrapy to obtain all the PDFs from MIDL but in the end any external source of PDFs can be used it doesn’t need to be automated.\nGROBID Finally, the last part of our data collection and processing is to convert all the PDFs we obtained from the previous steps into a format easier to parse. To do it we decided to use GROBID, a library to convert raw documents like PDFs into XML and works especially well for scientific documents so it’s the perfect tool for our study. GROBID extracts the information from articles using a cascade of models to first divide the papers into main areas like title page, body, footnotes or reference section and then apply specific models for each area to re-structure the information in an XML format.\nHere is an example of data obtained from the conversion:\nThe XML preserve information on the sections as you can see in the red boxes with a division for the introduction and at the bottom the start of the division of the method section. The conservation of sections\u0026rsquo; names is important as we believe only mentions from relevant parts of the full text like methods or data part indicate a usage, therefore we only extract mentions in these parts. Finally, GROBID can also handle the reference section and we used it to complement the matching done with OpenAlex to avoid relying on a single source for each information. Now that we have all the data, we can make the matching of regular expressions to detect the mentions of the datasets.\nPDF annotation tool After the detection we evaluated the efficiency of our pipeline by screening some papers, we therefore had to annotate the PDFs but we couldn’t find the perfect tool for it as some alternatives like Taguette and LabelStudio had some major limitations for us like the conversion from PDFs to raw text for Taguette or the lack of free features for multiple annotators with Label Studio. We therefore developed our own annotation tools with Streamlit that allows multiple annotators for the same files and keeps the PDF format. You can also divide the papers into different groups to ease the division of annotations among the different annotators. As I said we applied this tool in our study to verify the location of different mentions in the paper but it can be applied to any PDF annotation task and it’s fully open-source and free so you can use it for your projects.\nResults Citations and mentions practices Now that we have a complete view of our pipeline let\u0026rsquo;s talk about our findings. First I wanted to show why we needed such a process in the first place. I talked about dataset presence, but this presence takes very different forms. For example, only looking at the different citation practices, the most classical approach is like with the dataset name and the citations to the papers.\nHowever, you can sometimes have citations without the name of datasets, this kind of citation is a problem for our study on the usage because this paper isn’t using the dataset while citing the paper.\nFinally, there are also bad citations, where the authors are not following the dataset guidelines like in the following image only putting the link to BRATS in the citations and not the papers as asked by the dataset guideline.\nThe same goes for the mentions, with various styles. We had the classical mention previously but there are also other possibilities like putting the dataset in the footnotes with the link or in specific structures like figures or tables.\nWe analyzed the different practices for the datasets we selected and could find 3 groups. We display the percentage of each group per venue and dataset in the next figure:\nThe first group is the \u0026ldquo;only cited\u0026rdquo;, as I said we hypothesise that when you only cite a dataset paper without mentioning its name, alias or URL in a meaningful part of the full text, there is no usage of the dataset itself but only its paper in the introduction or related works. Despite the variability, it often represents more than 25% of the paper for each dataset and venue. The second problematic group is the “only mentioned”, this group doesn’t cite the dataset paper but only mentions it or doesn’t follow the dataset guidelines as we discussed previously. It’s a problem because classical tools for citation tracking can’t detect these papers, it represents more than 10% of papers in most cases. Finally, there is the “cited and mentioned” group which is sort of the base case scenario. This highlights the lack of a standard way to acknowledge dataset usage in papers.\nPopularity Now let’s compare the different datasets to see if some are more present and used than others. We selected 20 datasets both for segmentation and classification with various organs. We selected multiple datasets for the cardiac segmentation and the chest x-ray classification tasks to evaluate the diversity for a similar task.\nWe found very similar results for both segmentation and classification as displayed in the figures, they show the cumulative citation in full line and mention in dash line across years:\nWe only selected some datasets to highlight our findings. But our first finding is actually that most of the 20 datasets were almost not cited or mentioned at all showing the concentration of research on a few datasets as we hypothesis at the beginning of our study. Finally, we can also see a lack of diversity for the same task. Focusing on the segmentation datasets, we can obviously see that BRATS is by far the most present dataset but the main point here is that ACDC and M\u0026amp;Ms which are both cardiac segmentation datasets have very different results. ACDC still receives more citations and mentions despite being released before M\u0026amp;Ms while M\u0026amp;Ms already stagnating.\nRecommendations We derived from our study a set of recommendations:\nFirst for dataset providers, releasing a paper associated with the dataset will be very beneficial for the user to have a resource easy to cite to benefit from the classical citation tracking tools. Also, choosing a proper and unique name for your dataset is very important, a \u0026ldquo;funny\u0026rdquo; finding from our work is the usage by 3 different datasets of the acronym \u0026ldquo;ACDC\u0026rdquo;, 2 from the medical domain.\nThen for authors and the community, we should really document the dataset we use and follow the provider\u0026rsquo;s guidelines. While this may seem obvious, our research showed that is not always done. Finally, we should work toward a standard way to acknowledge dataset usage. We could for example take inspiration from some journals and venues asking for specific a \u0026ldquo;data availability\u0026rdquo; section in their guidelines.\n","date":"July 9, 2024","hero":"/posts/my-research/citation-needed/images/preview.png","permalink":"https://theosourget.github.io/posts/my-research/citation-needed/","summary":"Click on the image below to see my oral session at MIDL 2024:\nNowadays, the evaluation of models heavily relies on publicly available datasets used as benchmarking. While this could be a nice thing for a fair comparison of different models, we also question the effect of the diversity or more precisely a potential lack of diversity in research papers when selecting the datasets. A gap has been observed between the results showcased by AI models in research and their adoption in clinical workflow, we hypothesise that this gap could partly be a result of an overfitting of research on these datasets and we wanted to evaluate their usage to know if some are more popular than others.","tags":null,"title":"[Citation needed] Data usage and citation practices in medical imaging conferences​"}]