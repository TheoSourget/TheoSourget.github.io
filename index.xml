<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Theo Sourget</title><link>https://theosourget.github.io/</link><description>Recent content on Theo Sourget</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 20 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://theosourget.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Create your own applications with Streamlit, FastAPI and Docker​</title><link>https://theosourget.github.io/posts/other/create-your-own-app/</link><pubDate>Sun, 20 Apr 2025 00:00:00 +0000</pubDate><guid>https://theosourget.github.io/posts/other/create-your-own-app/</guid><description>&lt;p>During my projects, I like to build concrete applications from the research I conduct. I use them to demonstrate the capacity of a trained model or a pipeline I developed but also use the code in a more general purpose. To build these apps I use a combination of three tools: &lt;a href="https://streamlit.io/" target="_blank" rel="noopener">Streamlit&lt;/a>, &lt;a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener">FastAPI&lt;/a>, and &lt;a href="https://www.docker.com/" target="_blank" rel="noopener">Docker&lt;/a>. In this post, I first show two examples of applications I&amp;rsquo;ve created, and then I present the tools with some simple examples of code.&lt;/p></description></item><item><title>[Citation needed] Data usage and citation practices in medical imaging conferences​</title><link>https://theosourget.github.io/posts/my-research/citation-needed/</link><pubDate>Tue, 09 Jul 2024 00:00:00 +0000</pubDate><guid>https://theosourget.github.io/posts/my-research/citation-needed/</guid><description>&lt;p>&lt;strong>Click on the image below to see my oral session at MIDL 2024:&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/live/-mV53dZZs9o?t=20616s" target="_blank" rel="noopener">&lt;img src="https://img.youtube.com/vi/-mV53dZZs9o/0.jpg" alt="Link to MIDL presentation video">&lt;/a>&lt;/p>
&lt;p>Nowadays, the evaluation of models heavily relies on publicly available datasets used as benchmarking. While this could be a nice thing for a fair comparison of different models, we also question the effect of the diversity or more precisely a potential lack of diversity in research papers when selecting the datasets. A gap has been observed between the results showcased by AI models in research and their adoption in clinical workflow, we hypothesise that this gap could partly be a result of an overfitting of research on these datasets and we wanted to evaluate their usage to know if some are more popular than others. While this could seem like a straightforward task we’ll see that because of some elements it turned out to be not so simple.&lt;/p></description></item></channel></rss>